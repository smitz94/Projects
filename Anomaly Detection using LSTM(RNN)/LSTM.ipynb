{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SMIT ZAVERI\\Anaconda3\\envs\\web scrapping\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\SMIT ZAVERI\\Anaconda3\\envs\\web scrapping\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\SMIT ZAVERI\\Anaconda3\\envs\\web scrapping\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\SMIT ZAVERI\\Anaconda3\\envs\\web scrapping\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\SMIT ZAVERI\\Anaconda3\\envs\\web scrapping\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\SMIT ZAVERI\\Anaconda3\\envs\\web scrapping\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 4)                 96        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 101\n",
      "Trainable params: 101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\SMIT ZAVERI\\Anaconda3\\envs\\web scrapping\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Epoch 1/150\n",
      " - 5s - loss: 0.3963 - binary_accuracy: 0.8987\n",
      "Epoch 2/150\n",
      " - 4s - loss: 0.3273 - binary_accuracy: 0.9002\n",
      "Epoch 3/150\n",
      " - 4s - loss: 0.3255 - binary_accuracy: 0.9002\n",
      "Epoch 4/150\n",
      " - 4s - loss: 0.3242 - binary_accuracy: 0.9002\n",
      "Epoch 5/150\n",
      " - 4s - loss: 0.3232 - binary_accuracy: 0.9002\n",
      "Epoch 6/150\n",
      " - 4s - loss: 0.3226 - binary_accuracy: 0.9002\n",
      "Epoch 7/150\n",
      " - 4s - loss: 0.3222 - binary_accuracy: 0.9002\n",
      "Epoch 8/150\n",
      " - 4s - loss: 0.3219 - binary_accuracy: 0.9002\n",
      "Epoch 9/150\n",
      " - 4s - loss: 0.3218 - binary_accuracy: 0.9002\n",
      "Epoch 10/150\n",
      " - 4s - loss: 0.3218 - binary_accuracy: 0.9002\n",
      "Epoch 11/150\n",
      " - 4s - loss: 0.3217 - binary_accuracy: 0.9002\n",
      "Epoch 12/150\n",
      " - 4s - loss: 0.3217 - binary_accuracy: 0.9002\n",
      "Epoch 13/150\n",
      " - 4s - loss: 0.3217 - binary_accuracy: 0.9002\n",
      "Epoch 14/150\n",
      " - 4s - loss: 0.3217 - binary_accuracy: 0.9002\n",
      "Epoch 15/150\n",
      " - 4s - loss: 0.3217 - binary_accuracy: 0.9002\n",
      "Epoch 16/150\n",
      " - 4s - loss: 0.3216 - binary_accuracy: 0.9002\n",
      "Epoch 17/150\n",
      " - 4s - loss: 0.3216 - binary_accuracy: 0.9002\n",
      "Epoch 18/150\n",
      " - 4s - loss: 0.3216 - binary_accuracy: 0.9002\n",
      "Epoch 19/150\n",
      " - 4s - loss: 0.3216 - binary_accuracy: 0.9002\n",
      "Epoch 20/150\n",
      " - 4s - loss: 0.3217 - binary_accuracy: 0.9002\n",
      "Epoch 21/150\n",
      " - 4s - loss: 0.3217 - binary_accuracy: 0.9002\n",
      "Epoch 22/150\n",
      " - 4s - loss: 0.3216 - binary_accuracy: 0.9002\n",
      "Epoch 23/150\n",
      " - 4s - loss: 0.3216 - binary_accuracy: 0.9002\n",
      "Epoch 24/150\n",
      " - 4s - loss: 0.3216 - binary_accuracy: 0.9002\n",
      "Epoch 25/150\n",
      " - 4s - loss: 0.3215 - binary_accuracy: 0.9002\n",
      "Epoch 26/150\n",
      " - 4s - loss: 0.3216 - binary_accuracy: 0.9002\n",
      "Epoch 27/150\n",
      " - 4s - loss: 0.3216 - binary_accuracy: 0.9002\n",
      "Epoch 28/150\n",
      " - 4s - loss: 0.3216 - binary_accuracy: 0.9002\n",
      "Epoch 29/150\n",
      " - 4s - loss: 0.3216 - binary_accuracy: 0.9002\n",
      "Epoch 30/150\n",
      " - 4s - loss: 0.3216 - binary_accuracy: 0.9002\n",
      "Epoch 31/150\n",
      " - 4s - loss: 0.3215 - binary_accuracy: 0.9002\n",
      "Epoch 32/150\n",
      " - 4s - loss: 0.3216 - binary_accuracy: 0.9002\n",
      "Epoch 33/150\n",
      " - 4s - loss: 0.3216 - binary_accuracy: 0.9002\n",
      "Epoch 34/150\n",
      " - 4s - loss: 0.3216 - binary_accuracy: 0.9002\n",
      "Epoch 35/150\n",
      " - 4s - loss: 0.3216 - binary_accuracy: 0.9002\n",
      "Epoch 36/150\n",
      " - 4s - loss: 0.3215 - binary_accuracy: 0.9002\n",
      "Epoch 37/150\n",
      " - 4s - loss: 0.3216 - binary_accuracy: 0.9002\n",
      "Epoch 38/150\n",
      " - 4s - loss: 0.3216 - binary_accuracy: 0.9002\n",
      "Epoch 39/150\n",
      " - 4s - loss: 0.3215 - binary_accuracy: 0.9002\n",
      "Epoch 40/150\n",
      " - 4s - loss: 0.3215 - binary_accuracy: 0.9002\n",
      "Epoch 41/150\n",
      " - 4s - loss: 0.3216 - binary_accuracy: 0.9002\n",
      "Epoch 42/150\n",
      " - 4s - loss: 0.3215 - binary_accuracy: 0.9002\n",
      "Epoch 43/150\n",
      " - 4s - loss: 0.3215 - binary_accuracy: 0.9002\n",
      "Epoch 44/150\n",
      " - 4s - loss: 0.3215 - binary_accuracy: 0.9002\n",
      "Epoch 45/150\n",
      " - 4s - loss: 0.3215 - binary_accuracy: 0.9002\n",
      "Epoch 46/150\n",
      " - 4s - loss: 0.3215 - binary_accuracy: 0.9002\n",
      "Epoch 47/150\n",
      " - 4s - loss: 0.3215 - binary_accuracy: 0.9002\n",
      "Epoch 48/150\n",
      " - 4s - loss: 0.3215 - binary_accuracy: 0.9002\n",
      "Epoch 49/150\n",
      " - 4s - loss: 0.3215 - binary_accuracy: 0.9002\n",
      "Epoch 50/150\n",
      " - 5s - loss: 0.3215 - binary_accuracy: 0.9002\n",
      "Epoch 51/150\n",
      " - 4s - loss: 0.3215 - binary_accuracy: 0.9002\n",
      "Epoch 52/150\n",
      " - 5s - loss: 0.3214 - binary_accuracy: 0.9002\n",
      "Epoch 53/150\n",
      " - 4s - loss: 0.3215 - binary_accuracy: 0.9002\n",
      "Epoch 54/150\n",
      " - 4s - loss: 0.3215 - binary_accuracy: 0.9002\n",
      "Epoch 55/150\n",
      " - 5s - loss: 0.3214 - binary_accuracy: 0.9002\n",
      "Epoch 56/150\n",
      " - 4s - loss: 0.3215 - binary_accuracy: 0.9002\n",
      "Epoch 57/150\n",
      " - 4s - loss: 0.3215 - binary_accuracy: 0.9002\n",
      "Epoch 58/150\n",
      " - 4s - loss: 0.3215 - binary_accuracy: 0.9002\n",
      "Epoch 59/150\n",
      " - 4s - loss: 0.3215 - binary_accuracy: 0.9002\n",
      "Epoch 60/150\n",
      " - 4s - loss: 0.3215 - binary_accuracy: 0.9002\n",
      "Epoch 61/150\n",
      " - 4s - loss: 0.3214 - binary_accuracy: 0.9002\n",
      "Epoch 62/150\n",
      " - 4s - loss: 0.3215 - binary_accuracy: 0.9002\n",
      "Epoch 63/150\n",
      " - 4s - loss: 0.3215 - binary_accuracy: 0.9002\n",
      "Epoch 64/150\n",
      " - 4s - loss: 0.3214 - binary_accuracy: 0.9002\n",
      "Epoch 65/150\n",
      " - 5s - loss: 0.3213 - binary_accuracy: 0.9002\n",
      "Epoch 66/150\n",
      " - 4s - loss: 0.3214 - binary_accuracy: 0.9002\n",
      "Epoch 67/150\n",
      " - 5s - loss: 0.3214 - binary_accuracy: 0.9002\n",
      "Epoch 68/150\n",
      " - 4s - loss: 0.3213 - binary_accuracy: 0.9002\n",
      "Epoch 69/150\n",
      " - 4s - loss: 0.3213 - binary_accuracy: 0.9002\n",
      "Epoch 70/150\n",
      " - 4s - loss: 0.3213 - binary_accuracy: 0.9002\n",
      "Epoch 71/150\n",
      " - 4s - loss: 0.3213 - binary_accuracy: 0.9002\n",
      "Epoch 72/150\n",
      " - 4s - loss: 0.3213 - binary_accuracy: 0.9002\n",
      "Epoch 73/150\n",
      " - 5s - loss: 0.3213 - binary_accuracy: 0.9002\n",
      "Epoch 74/150\n",
      " - 4s - loss: 0.3212 - binary_accuracy: 0.9002\n",
      "Epoch 75/150\n",
      " - 4s - loss: 0.3213 - binary_accuracy: 0.9002\n",
      "Epoch 76/150\n",
      " - 4s - loss: 0.3212 - binary_accuracy: 0.9002\n",
      "Epoch 77/150\n",
      " - 4s - loss: 0.3213 - binary_accuracy: 0.9002\n",
      "Epoch 78/150\n",
      " - 5s - loss: 0.3213 - binary_accuracy: 0.9002\n",
      "Epoch 79/150\n",
      " - 4s - loss: 0.3212 - binary_accuracy: 0.9002\n",
      "Epoch 80/150\n",
      " - 5s - loss: 0.3213 - binary_accuracy: 0.9002\n",
      "Epoch 81/150\n",
      " - 5s - loss: 0.3212 - binary_accuracy: 0.9002\n",
      "Epoch 82/150\n",
      " - 4s - loss: 0.3212 - binary_accuracy: 0.9002\n",
      "Epoch 83/150\n",
      " - 4s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 84/150\n",
      " - 4s - loss: 0.3212 - binary_accuracy: 0.9002\n",
      "Epoch 85/150\n",
      " - 4s - loss: 0.3213 - binary_accuracy: 0.9002\n",
      "Epoch 86/150\n",
      " - 4s - loss: 0.3212 - binary_accuracy: 0.9002\n",
      "Epoch 87/150\n",
      " - 4s - loss: 0.3213 - binary_accuracy: 0.9002\n",
      "Epoch 88/150\n",
      " - 4s - loss: 0.3212 - binary_accuracy: 0.9002\n",
      "Epoch 89/150\n",
      " - 5s - loss: 0.3213 - binary_accuracy: 0.9002\n",
      "Epoch 90/150\n",
      " - 4s - loss: 0.3212 - binary_accuracy: 0.9002\n",
      "Epoch 91/150\n",
      " - 4s - loss: 0.3213 - binary_accuracy: 0.9002\n",
      "Epoch 92/150\n",
      " - 4s - loss: 0.3212 - binary_accuracy: 0.9002\n",
      "Epoch 93/150\n",
      " - 5s - loss: 0.3212 - binary_accuracy: 0.9002\n",
      "Epoch 94/150\n",
      " - 4s - loss: 0.3212 - binary_accuracy: 0.9002\n",
      "Epoch 95/150\n",
      " - 4s - loss: 0.3212 - binary_accuracy: 0.9002\n",
      "Epoch 96/150\n",
      " - 5s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 97/150\n",
      " - 4s - loss: 0.3212 - binary_accuracy: 0.9002\n",
      "Epoch 98/150\n",
      " - 4s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 99/150\n",
      " - 4s - loss: 0.3212 - binary_accuracy: 0.9002\n",
      "Epoch 100/150\n",
      " - 5s - loss: 0.3212 - binary_accuracy: 0.9002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/150\n",
      " - 4s - loss: 0.3212 - binary_accuracy: 0.9002\n",
      "Epoch 102/150\n",
      " - 4s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 103/150\n",
      " - 5s - loss: 0.3212 - binary_accuracy: 0.9002\n",
      "Epoch 104/150\n",
      " - 5s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 105/150\n",
      " - 4s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 106/150\n",
      " - 4s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 107/150\n",
      " - 4s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 108/150\n",
      " - 4s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 109/150\n",
      " - 4s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 110/150\n",
      " - 4s - loss: 0.3210 - binary_accuracy: 0.9002\n",
      "Epoch 111/150\n",
      " - 4s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 112/150\n",
      " - 5s - loss: 0.3212 - binary_accuracy: 0.9002\n",
      "Epoch 113/150\n",
      " - 4s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 114/150\n",
      " - 4s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 115/150\n",
      " - 5s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 116/150\n",
      " - 4s - loss: 0.3212 - binary_accuracy: 0.9002\n",
      "Epoch 117/150\n",
      " - 4s - loss: 0.3209 - binary_accuracy: 0.9002\n",
      "Epoch 118/150\n",
      " - 4s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 119/150\n",
      " - 5s - loss: 0.3210 - binary_accuracy: 0.9002\n",
      "Epoch 120/150\n",
      " - 5s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 121/150\n",
      " - 5s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 122/150\n",
      " - 5s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 123/150\n",
      " - 4s - loss: 0.3210 - binary_accuracy: 0.9002\n",
      "Epoch 124/150\n",
      " - 4s - loss: 0.3210 - binary_accuracy: 0.9002\n",
      "Epoch 125/150\n",
      " - 4s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 126/150\n",
      " - 5s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 127/150\n",
      " - 4s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 128/150\n",
      " - 4s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 129/150\n",
      " - 5s - loss: 0.3210 - binary_accuracy: 0.9002\n",
      "Epoch 130/150\n",
      " - 4s - loss: 0.3210 - binary_accuracy: 0.9002\n",
      "Epoch 131/150\n",
      " - 5s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 132/150\n",
      " - 4s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 133/150\n",
      " - 5s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 134/150\n",
      " - 4s - loss: 0.3210 - binary_accuracy: 0.9002\n",
      "Epoch 135/150\n",
      " - 5s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 136/150\n",
      " - 4s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 137/150\n",
      " - 4s - loss: 0.3210 - binary_accuracy: 0.9002\n",
      "Epoch 138/150\n",
      " - 4s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 139/150\n",
      " - 4s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 140/150\n",
      " - 4s - loss: 0.3210 - binary_accuracy: 0.9002\n",
      "Epoch 141/150\n",
      " - 4s - loss: 0.3209 - binary_accuracy: 0.9002\n",
      "Epoch 142/150\n",
      " - 4s - loss: 0.3210 - binary_accuracy: 0.9002\n",
      "Epoch 143/150\n",
      " - 5s - loss: 0.3210 - binary_accuracy: 0.9002\n",
      "Epoch 144/150\n",
      " - 5s - loss: 0.3210 - binary_accuracy: 0.9002\n",
      "Epoch 145/150\n",
      " - 4s - loss: 0.3211 - binary_accuracy: 0.9002\n",
      "Epoch 146/150\n",
      " - 5s - loss: 0.3208 - binary_accuracy: 0.9002\n",
      "Epoch 147/150\n",
      " - 4s - loss: 0.3210 - binary_accuracy: 0.9002\n",
      "Epoch 148/150\n",
      " - 5s - loss: 0.3210 - binary_accuracy: 0.9002\n",
      "Epoch 149/150\n",
      " - 5s - loss: 0.3210 - binary_accuracy: 0.9002\n",
      "Epoch 150/150\n",
      " - 5s - loss: 0.3210 - binary_accuracy: 0.9002\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.72      0.81     14519\n",
      "         1.0       0.13      0.36      0.19      1609\n",
      "\n",
      "    accuracy                           0.69     16128\n",
      "   macro avg       0.52      0.54      0.50     16128\n",
      "weighted avg       0.83      0.69      0.75     16128\n",
      "\n",
      "[[10514  4005]\n",
      " [ 1028   581]]\n",
      "F1 Score= 0.18757062146892656\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# fixing the randomness to reproduce the data for debugging\n",
    "np.random.seed(7)\n",
    "\n",
    "# read all 4 input files\n",
    "data=pd.read_csv('Documents\\ec2_cpu_utilization_24ae8d.csv')\n",
    "\n",
    "df=pd.DataFrame(data)\n",
    "df1=pd.read_csv('Documents\\ec2_cpu_utilization_5f5533.csv')\n",
    "df2=pd.read_csv('Documents\\ec2_cpu_utilization_53ea38.csv')\n",
    "df3=pd.read_csv('Documents\\ec2_cpu_utilization_77c1ca.csv')\n",
    "\n",
    "#Normalizing data to scale it for the training\n",
    "df3_trans=df3.drop('timestamp', 1)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df3_trans = scaler.fit_transform(df3_trans)\n",
    "\n",
    "df3_trans=pd.DataFrame(df3_trans)\n",
    "df3_trans.columns=['value','label']\n",
    "\n",
    "\n",
    "df2_trans=df2.drop('timestamp', 1)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df2_trans = scaler.fit_transform(df2_trans)\n",
    "\n",
    "df2_trans=pd.DataFrame(df2_trans)\n",
    "df2_trans.columns=['value','label']\n",
    "\n",
    "df1_trans=df1.drop('timestamp', 1)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df1_trans = scaler.fit_transform(df1_trans)\n",
    "\n",
    "df1_trans=pd.DataFrame(df1_trans)\n",
    "df1_trans.columns=['value','label']\n",
    "\n",
    "# merge all the data for training\n",
    "frames=[df,df1_trans,df2_trans,df3_trans]\n",
    "df_final = pd.concat(frames,sort=False)\n",
    "\n",
    "\n",
    "# feature engineering (applying moving average over a window size of 10 elements)\n",
    "X_in=df_final['value']\n",
    "Y_in=df_final['label']\n",
    "X_in=X_in.values\n",
    "\n",
    "MX=df_final['value'].rolling(window=10).mean()\n",
    "MX=pd.DataFrame(MX)\n",
    "\n",
    "MX.columns=['avg']\n",
    "MX.loc[MX['avg'].isnull(),'avg'] = .132\n",
    "\n",
    "MX=MX.values\n",
    "\n",
    "#reshaping data for training\n",
    "trainX=X_in.reshape((X_in.shape[0],1,1))\n",
    "\n",
    "trainmx=MX.reshape((MX.shape[0],1,1))\n",
    "\n",
    "# training the model and predicting the output the same data to preserve the pattern in the data\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(trainmx.shape[1], trainmx.shape[2])))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(trainmx,Y_in, epochs=150, batch_size=10, verbose=2)\n",
    "\n",
    "#predicting data and grouping the alert\n",
    "mypred = model.predict(trainmx)\n",
    "\n",
    "avg =np.mean(mypred)\n",
    "#grouping the alert based on mean and distributing them according to it\n",
    "mypred1=[]\n",
    "for i in mypred:\n",
    "    if(i>=avg):\n",
    "        mypred1.append(1)\n",
    "    else:\n",
    "        mypred1.append(0)\n",
    "\n",
    "mypred1=np.array(mypred1)\n",
    "\n",
    "# Evaluating model\n",
    "print(classification_report(Y_in, mypred1))\n",
    "print(confusion_matrix(Y_in,mypred1))\n",
    "print(\"F1 Score=\",f1_score(Y_in,mypred1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating predictions on the Merged file 1 by 1 and see the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. For the data \"ec2_cpu_utilization_5f5533.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.65      0.76      3630\n",
      "         1.0       0.13      0.49      0.21       402\n",
      "\n",
      "    accuracy                           0.63      4032\n",
      "   macro avg       0.53      0.57      0.48      4032\n",
      "weighted avg       0.84      0.63      0.70      4032\n",
      "\n",
      "[[2343 1287]\n",
      " [ 207  195]]\n",
      "F1 Score= 0.20700636942675157\n"
     ]
    }
   ],
   "source": [
    "X1_in=df1_trans['value'].values\n",
    "Y1_in=df1_trans['label'].values\n",
    "\n",
    "\n",
    "MX1=df1_trans['value'].rolling(window=10).mean()\n",
    "MX1=pd.DataFrame(MX1)\n",
    "\n",
    "MX1.columns=['avg']\n",
    "MX1.loc[MX1['avg'].isnull(),'avg'] = X1_in[0]\n",
    "\n",
    "MX1=MX1.values\n",
    "\n",
    "trainmx1=MX1.reshape((MX1.shape[0],1,1))\n",
    "\n",
    "mypred = model.predict(trainmx1)\n",
    "\n",
    "avg =np.mean(mypred)\n",
    "\n",
    "mypred1=[]\n",
    "for i in mypred:\n",
    "    if(i>=avg):\n",
    "        mypred1.append(1)\n",
    "    else:\n",
    "        mypred1.append(0)\n",
    "\n",
    "mypred1=np.array(mypred1)\n",
    "\n",
    "\n",
    "print(classification_report(Y1_in, mypred1))\n",
    "print(confusion_matrix(Y1_in,mypred1))\n",
    "print(\"F1 Score=\",f1_score(Y1_in,mypred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. For the data \"ec2_cpu_utilization_53ea38.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.65      0.76      3630\n",
      "         1.0       0.12      0.44      0.19       402\n",
      "\n",
      "    accuracy                           0.63      4032\n",
      "   macro avg       0.52      0.54      0.47      4032\n",
      "weighted avg       0.83      0.63      0.70      4032\n",
      "\n",
      "[[2349 1281]\n",
      " [ 224  178]]\n",
      "F1 Score= 0.19129500268672758\n"
     ]
    }
   ],
   "source": [
    "X2_in=df2_trans['value'].values\n",
    "Y2_in=df2_trans['label'].values\n",
    "\n",
    "MX2=df2_trans['value'].rolling(window=10).mean()\n",
    "MX2=pd.DataFrame(MX2)\n",
    "\n",
    "MX2.columns=['avg']\n",
    "MX2.loc[MX2['avg'].isnull(),'avg'] = X2_in[0]\n",
    "\n",
    "MX2=MX2.values\n",
    "\n",
    "trainmx2=MX2.reshape((MX2.shape[0],1,1))\n",
    "\n",
    "mypred = model.predict(trainmx2)\n",
    "avg =np.mean(mypred)\n",
    "\n",
    "mypred1=[]\n",
    "for i in mypred:\n",
    "    if(i>=avg):\n",
    "        mypred1.append(1)\n",
    "    else:\n",
    "        mypred1.append(0)\n",
    "\n",
    "mypred1=np.array(mypred1)\n",
    "\n",
    "\n",
    "print(classification_report(Y2_in, mypred1))\n",
    "print(confusion_matrix(Y2_in,mypred1))\n",
    "print(\"F1 Score=\",f1_score(Y2_in,mypred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. For the data \"ec2_cpu_utilization_77c1ca.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.82      0.86      3629\n",
      "         1.0       0.16      0.32      0.22       403\n",
      "\n",
      "    accuracy                           0.77      4032\n",
      "   macro avg       0.54      0.57      0.54      4032\n",
      "weighted avg       0.84      0.77      0.80      4032\n",
      "\n",
      "[[2973  656]\n",
      " [ 275  128]]\n",
      "F1 Score= 0.2156697556866049\n"
     ]
    }
   ],
   "source": [
    "X3_in=df3_trans['value'].values\n",
    "Y3_in=df3_trans['label'].values\n",
    "\n",
    "MX3=df3_trans['value'].rolling(window=10).mean()\n",
    "MX3=pd.DataFrame(MX3)\n",
    "\n",
    "MX3.columns=['avg']\n",
    "MX3.loc[MX3['avg'].isnull(),'avg'] = X3_in[0]\n",
    "\n",
    "MX3=MX3.values\n",
    "\n",
    "trainmx3=MX3.reshape((MX3.shape[0],1,1))\n",
    "\n",
    "mypred = model.predict(trainmx3)\n",
    "avg =np.mean(mypred)\n",
    "\n",
    "mypred1=[]\n",
    "for i in mypred:\n",
    "    if(i>=avg):\n",
    "        mypred1.append(1)\n",
    "    else:\n",
    "        mypred1.append(0)\n",
    "\n",
    "mypred1=np.array(mypred1)\n",
    "\n",
    "\n",
    "print(classification_report(Y3_in, mypred1))\n",
    "print(confusion_matrix(Y3_in,mypred1))\n",
    "print(\"F1 Score=\",f1_score(Y3_in,mypred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. For the data \"ec2_cpu_utilization_24ae8d.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93      3630\n",
      "           1       0.24      0.15      0.19       402\n",
      "\n",
      "    accuracy                           0.87      4032\n",
      "   macro avg       0.58      0.55      0.56      4032\n",
      "weighted avg       0.84      0.87      0.85      4032\n",
      "\n",
      "[[3441  189]\n",
      " [ 341   61]]\n",
      "F1 Score= 0.1871165644171779\n"
     ]
    }
   ],
   "source": [
    "X4_in=df['value'].values\n",
    "Y4_in=df['label'].values\n",
    "\n",
    "MX4=df['value'].rolling(window=10).mean()\n",
    "MX4=pd.DataFrame(MX4)\n",
    "\n",
    "MX4.columns=['avg']\n",
    "MX4.loc[MX4['avg'].isnull(),'avg'] = X4_in[0]\n",
    "\n",
    "MX4=MX4.values\n",
    "\n",
    "trainmx4=MX4.reshape((MX4.shape[0],1,1))\n",
    "\n",
    "mypred = model.predict(trainmx4)\n",
    "avg =np.mean(mypred)\n",
    "\n",
    "mypred1=[]\n",
    "for i in mypred:\n",
    "    if(i>=avg):\n",
    "        mypred1.append(1)\n",
    "    else:\n",
    "        mypred1.append(0)\n",
    "\n",
    "mypred1=np.array(mypred1)\n",
    "\n",
    "\n",
    "print(classification_report(Y4_in, mypred1))\n",
    "print(confusion_matrix(Y4_in,mypred1))\n",
    "print(\"F1 Score=\",f1_score(Y4_in,mypred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Output of ec2_cpu_utilization_24ae8d.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "After OverSampling, counts of label '1': 250\n",
      "After OverSampling, counts of label '0': 3782\n"
     ]
    }
   ],
   "source": [
    "data1=pd.read_csv('Documents\\ec2_cpu_utilization_24ae8d_1.csv')\n",
    "\n",
    "df1=pd.DataFrame(data1)\n",
    "\n",
    "X_in1=df1['value']\n",
    "\n",
    "X_in1=X_in1.values\n",
    "\n",
    "MX1=data1['value'].rolling(window=10).mean()\n",
    "MX1=pd.DataFrame(MX1)\n",
    "\n",
    "MX1.columns=['avg']\n",
    "MX1.loc[MX1['avg'].isnull(),'avg'] = X_in1[0]\n",
    "\n",
    "MX1=MX1.values\n",
    "\n",
    "\n",
    "trainX1=X_in1.reshape((X_in1.shape[0],1,1))\n",
    "\n",
    "trainmx1=MX1.reshape((MX1.shape[0],1,1))\n",
    "\n",
    "mypred = model.predict(trainmx1)\n",
    "\n",
    "avg =np.mean(mypred)\n",
    "\n",
    "mypred1=[]\n",
    "for i in mypred:\n",
    "    if(i>=avg):\n",
    "        mypred1.append(1)\n",
    "    else:\n",
    "        mypred1.append(0)\n",
    "\n",
    "mypred1=np.array(mypred1)\n",
    "print(mypred1)\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(mypred1 == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(mypred1 == 0))) \n",
    "\n",
    "df1['label']=mypred1\n",
    "df1.to_csv('Output_ec2_cpu_utilization_24ae8d.csv') \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Output of the Evaluation file \"ec2_cpu_utilization_5f5533.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 0 0 0]\n",
      "After OverSampling, counts of label '1': 1482\n",
      "After OverSampling, counts of label '0': 2550\n"
     ]
    }
   ],
   "source": [
    "# reading data\n",
    "data2=pd.read_csv('Documents\\Evaluation Dataset\\ec2_cpu_utilization_5f5533.csv')\n",
    "\n",
    "\n",
    "#Normalizing data for prediction\n",
    "df2=data2['value'].values.reshape(-1,1)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df2 = scaler.fit_transform(df2)\n",
    "\n",
    "df2=pd.DataFrame(df2)\n",
    "df2.columns=['value']\n",
    "\n",
    "X_in2=df2['value']\n",
    "X_in2=X_in2.values\n",
    "\n",
    "# Implementing moving average over window of 10\n",
    "MX2=df2['value'].rolling(window=10).mean()\n",
    "MX2=pd.DataFrame(MX2)\n",
    "\n",
    "MX2.columns=['avg']\n",
    "MX2.loc[MX2['avg'].isnull(),'avg'] = X_in2[0]\n",
    "\n",
    "MX2=MX2.values\n",
    "\n",
    "trainmx2=MX2.reshape((MX2.shape[0],1,1))\n",
    "\n",
    "# Predicting the output\n",
    "mypred = model.predict(trainmx2)\n",
    "\n",
    "avg =np.mean(mypred)\n",
    "\n",
    "mypred1=[]\n",
    "for i in mypred:\n",
    "    if(i>=avg):\n",
    "        mypred1.append(1)\n",
    "    else:\n",
    "        mypred1.append(0)\n",
    "\n",
    "mypred1=np.array(mypred1)\n",
    "print(mypred1)\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(mypred1 == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(mypred1 == 0))) \n",
    "\n",
    "# Saving the output in .csv file\n",
    "data2['label']=mypred1\n",
    "data2.to_csv('Output_ec2_cpu_utilization_5f5533.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Output of the Evaluation file \"ec2_cpu_utilization_53ea38.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "After OverSampling, counts of label '1': 1459\n",
      "After OverSampling, counts of label '0': 2573\n"
     ]
    }
   ],
   "source": [
    "# reading data\n",
    "data3=pd.read_csv('Documents\\Evaluation Dataset\\ec2_cpu_utilization_53ea38.csv')\n",
    "\n",
    "\n",
    "#Normalizing data for prediction\n",
    "df3=data3['value'].values.reshape(-1,1)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df3 = scaler.fit_transform(df3)\n",
    "\n",
    "df3=pd.DataFrame(df3)\n",
    "df3.columns=['value']\n",
    "\n",
    "X_in3=df3['value']\n",
    "X_in3=X_in3.values\n",
    "\n",
    "# Implementing moving average over window of 10\n",
    "MX3=df3['value'].rolling(window=10).mean()\n",
    "MX3=pd.DataFrame(MX3)\n",
    "\n",
    "MX3.columns=['avg']\n",
    "MX3.loc[MX3['avg'].isnull(),'avg'] = X_in3[0]\n",
    "\n",
    "MX3=MX3.values\n",
    "\n",
    "trainmx3=MX3.reshape((MX3.shape[0],1,1))\n",
    "\n",
    "# Predicting the output\n",
    "mypred = model.predict(trainmx3)\n",
    "\n",
    "avg =np.mean(mypred)\n",
    "\n",
    "mypred1=[]\n",
    "for i in mypred:\n",
    "    if(i>=avg):\n",
    "        mypred1.append(1)\n",
    "    else:\n",
    "        mypred1.append(0)\n",
    "\n",
    "mypred1=np.array(mypred1)\n",
    "print(mypred1)\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(mypred1 == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(mypred1 == 0))) \n",
    "\n",
    "# Saving the output in .csv file\n",
    "data3['label']=mypred1\n",
    "data3.to_csv('Output_ec2_cpu_utilization_53ea38.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Output of the Evaluation file \"ec2_cpu_utilization_77c1ca.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "After OverSampling, counts of label '1': 1459\n",
      "After OverSampling, counts of label '0': 2573\n"
     ]
    }
   ],
   "source": [
    "# reading data\n",
    "data4=pd.read_csv('Documents\\Evaluation Dataset\\ec2_cpu_utilization_77c1ca.csv')\n",
    "\n",
    "\n",
    "#Normalizing data for prediction\n",
    "df4=data4['value'].values.reshape(-1,1)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df4 = scaler.fit_transform(df4)\n",
    "\n",
    "df4=pd.DataFrame(df4)\n",
    "df4.columns=['value']\n",
    "\n",
    "X_in4=df4['value']\n",
    "X_in4=X_in4.values\n",
    "\n",
    "# Implementing moving average over window of 10\n",
    "MX4=df4['value'].rolling(window=10).mean()\n",
    "MX4=pd.DataFrame(MX4)\n",
    "\n",
    "MX4.columns=['avg']\n",
    "MX4.loc[MX4['avg'].isnull(),'avg'] = X_in4[0]\n",
    "\n",
    "MX4=MX4.values\n",
    "\n",
    "trainmx4=MX4.reshape((MX4.shape[0],1,1))\n",
    "\n",
    "# Predicting the output\n",
    "mypred = model.predict(trainmx2)\n",
    "\n",
    "avg =np.mean(mypred)\n",
    "\n",
    "mypred1=[]\n",
    "for i in mypred:\n",
    "    if(i>=avg):\n",
    "        mypred1.append(1)\n",
    "    else:\n",
    "        mypred1.append(0)\n",
    "\n",
    "mypred1=np.array(mypred1)\n",
    "print(mypred1)\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(mypred1 == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(mypred1 == 0))) \n",
    "\n",
    "# Saving the output in .csv file\n",
    "data4['label']=mypred1\n",
    "data4.to_csv('Output_ec2_cpu_utilization_77c1ca.csv') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
